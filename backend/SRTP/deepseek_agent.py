import os
import json
import pandas as pd
from openai import OpenAI

# ==============================================================================
# 0. é…ç½®å’Œå®¢æˆ·ç«¯è®¾ç½®
# ==============================================================================

# å®šä¹‰è¾“å‡ºç›®å½• - ç¡®ä¿ä¸Žmain.pyä¸­çš„outputsç›®å½•ä¸€è‡´
OUTPUT_DIR = "outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å®¢æˆ·ç«¯è®¾ç½®ï¼šè¿žæŽ¥åˆ° DeepSeek API
try:
    # ç¡®ä¿ä½ å·²ç»è®¾ç½®äº†çŽ¯å¢ƒå˜é‡ DEEPSEEK_API_KEY
    client = OpenAI(
        api_key=os.environ.get("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com/v1"
    )
except Exception as e:
    print("é”™è¯¯ï¼šè¯·ç¡®ä¿ä½ å·²ç»è®¾ç½®äº† DEEPSEEK_API_KEY çŽ¯å¢ƒå˜é‡ã€‚")
    exit()

# ==============================================================================
# 1. æˆ‘ä»¬çš„å·¥å…·å‡½æ•°ï¼šæ•°æ®é¢„å¤„ç† (æ— éœ€ä¿®æ”¹ï¼Œå¯ç›´æŽ¥ä½¿ç”¨)
# ==============================================================================
def preprocess_vehicle_data(filepath: str, point_type: str = None, start_time: str = None, end_time: str = None, bbox: list = None):
    """
    é¢„å¤„ç†XLSXæ ¼å¼çš„è½¦è¾†è½¨è¿¹ç‚¹æ•°æ®ã€‚
    å®ƒå¯ä»¥æ ¹æ®ç‚¹ç±»åž‹ï¼ˆèµ·ç‚¹/ç»ˆç‚¹ï¼‰ã€æ—¶é—´èŒƒå›´å’Œåœ°ç†è¾¹ç•Œæ¡†è¿›è¡Œç­›é€‰ã€‚
    
    :param filepath: åŽŸå§‹æ•°æ®çš„æ–‡ä»¶è·¯å¾„ (XLSXæ ¼å¼)ã€‚
    :param point_type: ç­›é€‰çš„ç‚¹ç±»åž‹ã€‚å¯é€‰å€¼ä¸º 'start' (èµ·ç‚¹), 'end' (ç»ˆç‚¹)ã€‚é»˜è®¤ä¸ºä¸è¿‡æ»¤ã€‚
    :param start_time: ç­›é€‰çš„å¼€å§‹æ—¶é—´æˆ³ (æ•´æ•°)ã€‚
    :param end_time: ç­›é€‰çš„ç»“æŸæ—¶é—´æˆ³ (æ•´æ•°)ã€‚
    :param bbox: åœ°ç†è¾¹ç•Œæ¡†ï¼Œæ ¼å¼ä¸º [min_lon, min_lat, max_lon, max_lat]ã€‚
    :return: ä¸€ä¸ªJSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å¤„ç†ç»“æžœçš„æ‘˜è¦ã€‚
    """
    def _convert_time_to_seconds(time_str: str) -> int:
        """å°† HH:MM:SS æˆ–ç§’å­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“æ—¥ç§’æ•°"""
        if time_str is None:
            return None
        try:
            return int(time_str)
        except ValueError:
            parts = list(map(int, time_str.split(':')))
            seconds = parts[0] * 3600
            if len(parts) > 1:
                seconds += parts[1] * 60
            if len(parts) > 2:
                seconds += parts[2]
            return seconds

    print(f"--- Pythonå‡½æ•° `preprocess_vehicle_data` è¢«æ‰§è¡Œ ---")
    print(f"å‚æ•°: filepath='{filepath}', point_type='{point_type}', start_time='{start_time}', end_time='{end_time}', bbox={bbox}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(filepath):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ '{filepath}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥æ–‡ä»¶åå’Œè·¯å¾„ã€‚")
        return json.dumps({"status": "error", "message": f"File not found: {filepath}"})

    try:
        column_names = ['timestamp', 'longitude', 'latitude', 'type', 'label']
        df = pd.read_excel(filepath, header=None, names=column_names, engine='openpyxl')
        original_rows = len(df)
        
        if point_type == 'start':
            df = df[df['type'] == 0]
        elif point_type == 'end':
            df = df[df['type'] == 1]
            
        start_seconds = _convert_time_to_seconds(start_time)
        end_seconds = _convert_time_to_seconds(end_time)

        if start_seconds is not None:
            df = df[df['timestamp'] >= start_seconds]
        if end_seconds is not None:
            df = df[df['timestamp'] <= end_seconds]
            
        if bbox and len(bbox) == 4:
            min_lon, min_lat, max_lon, max_lat = bbox
            df = df[
                (df['longitude'] >= min_lon) & (df['longitude'] <= max_lon) &
                (df['latitude'] >= min_lat) & (df['latitude'] <= max_lat)
            ]
            
        filtered_rows = len(df)
        output_filename = os.path.join(OUTPUT_DIR, "filtered_" + os.path.splitext(os.path.basename(filepath))[0] + ".csv")
        df.to_csv(output_filename, index=False)
        
        result = {
            "status": "success",
            "original_rows": original_rows,
            "filtered_rows": filtered_rows,
            "output_filepath": os.path.basename(output_filename),  # åªè¿”å›žæ–‡ä»¶å
            "filters_applied": {"point_type": point_type, "time_range": [start_time, end_time], "bbox": bbox}
        }
        
    except Exception as e:
        result = {"status": "error", "message": str(e)}
        
    return json.dumps(result)

def kmeans_cluster(input_filepath: str, n_clusters: int = 8, output_shapefile: str = "cluster_results.shp"):
    """
    å¯¹ç»™å®šçš„CSVæ•°æ®è¿›è¡ŒK-Meansèšç±»ï¼Œå¹¶è¾“å‡ºä¸ºShapefileã€‚

    :param input_filepath: è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„ (éœ€è¦åŒ…å«ç»çº¬åº¦åˆ—ï¼Œæ”¯æŒå¤šç§å‘½åæ–¹å¼)ã€‚
    :param n_clusters: è¦åˆ›å»ºçš„èšç±»æ•°é‡ (Kå€¼)ã€‚
    :param output_shapefile: è¾“å‡ºçš„Shapefileæ–‡ä»¶è·¯å¾„ã€‚
    :return: åŒ…å«èšç±»ç»“æžœæ‘˜è¦çš„JSONå­—ç¬¦ä¸²ã€‚
    """
    print(f"--- Pythonå‡½æ•° `kmeans_cluster` è¢«æ‰§è¡Œ ---")
    print(f"å‚æ•°: input_filepath='{input_filepath}', n_clusters={n_clusters}, output_shapefile='{output_shapefile}'")

    # ç¡®ä¿output_shapefileä¿å­˜åˆ°outputsç›®å½•
    if not os.path.dirname(output_shapefile):
        output_shapefile = os.path.join(OUTPUT_DIR, output_shapefile)

    def _find_coordinate_columns(df):
        """
        è‡ªåŠ¨è¯†åˆ«ç»çº¬åº¦åˆ—å
        è¿”å›ž (longitude_col, latitude_col) æˆ– (None, None) å¦‚æžœæœªæ‰¾åˆ°
        """
        # å¸¸è§çš„ç»åº¦åˆ—å
        longitude_aliases = ['longitude', 'lon', 'lng', 'long', 'x', 'X', 'Longitude', 'LON', 'LNG', 'LONG']
        # å¸¸è§çš„çº¬åº¦åˆ—å  
        latitude_aliases = ['latitude', 'lat', 'y', 'Y', 'Latitude', 'LAT']
        
        longitude_col = None
        latitude_col = None
        
        # æŸ¥æ‰¾ç»åº¦åˆ—
        for col in df.columns:
            if col in longitude_aliases:
                longitude_col = col
                break
        
        # æŸ¥æ‰¾çº¬åº¦åˆ—
        for col in df.columns:
            if col in latitude_aliases:
                latitude_col = col
                break
                
        return longitude_col, latitude_col

    try:
        from sklearn.cluster import KMeans
        import geopandas as gpd

        # è¯»å–æ•°æ®
        df = pd.read_csv(input_filepath)
        
        # è‡ªåŠ¨è¯†åˆ«ç»çº¬åº¦åˆ—
        longitude_col, latitude_col = _find_coordinate_columns(df)
        
        if longitude_col is None or latitude_col is None:
            available_columns = list(df.columns)
            return json.dumps({
                "status": "error", 
                "message": f"æ— æ³•è¯†åˆ«ç»çº¬åº¦åˆ—ã€‚å¯ç”¨åˆ—å: {available_columns}ã€‚æ”¯æŒçš„ç»åº¦åˆ—å: longitude, lon, lng, long, xã€‚æ”¯æŒçš„çº¬åº¦åˆ—å: latitude, lat, yã€‚"
            })
        
        print(f"   è¯†åˆ«åˆ°ç»åº¦åˆ—: '{longitude_col}', çº¬åº¦åˆ—: '{latitude_col}'")

        # æ‰§è¡ŒK-Meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=10)
        df['cluster'] = kmeans.fit_predict(df[[longitude_col, latitude_col]])

        # åˆ›å»ºGeoDataFrame
        gdf = gpd.GeoDataFrame(
            df, geometry=gpd.points_from_xy(df[longitude_col], df[latitude_col])
        )
        
        # è®¾ç½®åæ ‡å‚è€ƒç³»ç»Ÿ (WGS84)
        gdf.set_crs(epsg=4326, inplace=True)

        # ä¿å­˜ä¸ºShapefile
        gdf.to_file(output_shapefile, driver='ESRI Shapefile')

        # æ”¶é›†æ‰€æœ‰ç”Ÿæˆçš„ Shapefile ç›¸å…³æ–‡ä»¶ï¼ˆè¿”å›žç›¸å¯¹è·¯å¾„ï¼‰
        base_name = os.path.splitext(output_shapefile)[0]
        shapefile_extensions = ['.shp', '.shx', '.dbf', '.prj', '.cpg']
        generated_shapefile_files = []
        for ext in shapefile_extensions:
            file_path = base_name + ext
            if os.path.exists(file_path):
                # åªè¿”å›žæ–‡ä»¶åï¼Œä¸åŒ…å«è·¯å¾„
                generated_shapefile_files.append(os.path.basename(file_path))

        # å‡†å¤‡ç»“æžœæ‘˜è¦
        cluster_summary = df['cluster'].value_counts().to_dict()
        result = {
            "status": "success",
            "output_filepath": os.path.basename(output_shapefile), # åªè¿”å›žæ–‡ä»¶å
            "generated_files": generated_shapefile_files, # æ·»åŠ æ‰€æœ‰ç›¸å…³æ–‡ä»¶çš„åˆ—è¡¨
            "n_clusters": n_clusters,
            "cluster_point_counts": cluster_summary,
            "coordinate_columns_used": {"longitude": longitude_col, "latitude": latitude_col}
        }

    except ImportError as e:
        result = {"status": "error", "message": f"Missing required library: {e}. Please install scikit-learn and geopandas."}
    except Exception as e:
        result = {"status": "error", "message": str(e)}

    return json.dumps(result)
def create_heatmap(input_filepath: str, output_image_path: str = "heatmap.png", map_title: str = "Taxies Hotspot Analysis Heatmap"):
    """
    æ ¹æ®è¾“å…¥çš„CSVç‚¹æ•°æ®ç”Ÿæˆä¸€å¼ å¸¦æœ‰åº•å›¾çš„çƒ­åŠ›å›¾ã€‚

    :param input_filepath: è¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„ (éœ€è¦åŒ…å«ç»çº¬åº¦åˆ—ï¼Œæ”¯æŒå¤šç§å‘½åæ–¹å¼)ã€‚
    :param output_image_path: è¾“å‡ºçš„çƒ­åŠ›å›¾å›¾ç‰‡æ–‡ä»¶è·¯å¾„ (PNGæ ¼å¼)ã€‚
    :param map_title: åœ°å›¾çš„æ ‡é¢˜ã€‚
    :return: åŒ…å«æ“ä½œçŠ¶æ€å’Œå›¾ç‰‡è·¯å¾„çš„JSONå­—ç¬¦ä¸²ã€‚
    """
    print(f"--- Pythonå‡½æ•° `create_heatmap` è¢«æ‰§è¡Œ ---")
    print(f"å‚æ•°: input_filepath='{input_filepath}', output_image_path='{output_image_path}', map_title='{map_title}'")

    # ç¡®ä¿output_image_pathä¿å­˜åˆ°outputsç›®å½•
    if not os.path.dirname(output_image_path):
        output_image_path = os.path.join(OUTPUT_DIR, output_image_path)

    def _find_coordinate_columns(df):
        """
        è‡ªåŠ¨è¯†åˆ«ç»çº¬åº¦åˆ—å
        è¿”å›ž (longitude_col, latitude_col) æˆ– (None, None) å¦‚æžœæœªæ‰¾åˆ°
        """
        # å¸¸è§çš„ç»åº¦åˆ—å
        longitude_aliases = ['longitude', 'lon', 'lng', 'long', 'x', 'X', 'Longitude', 'LON', 'LNG', 'LONG']
        # å¸¸è§çš„çº¬åº¦åˆ—å  
        latitude_aliases = ['latitude', 'lat', 'y', 'Y', 'Latitude', 'LAT']
        
        longitude_col = None
        latitude_col = None
        
        # æŸ¥æ‰¾ç»åº¦åˆ—
        for col in df.columns:
            if col in longitude_aliases:
                longitude_col = col
                break
        
        # æŸ¥æ‰¾çº¬åº¦åˆ—
        for col in df.columns:
            if col in latitude_aliases:
                latitude_col = col
                break
                
        return longitude_col, latitude_col

    try:
        import geopandas as gpd
        import matplotlib.pyplot as plt
        import contextily as ctx
        import seaborn as sns

        # ä»ŽCSVåˆ›å»ºGeoDataFrame
        df = pd.read_csv(input_filepath)
        
        # è‡ªåŠ¨è¯†åˆ«ç»çº¬åº¦åˆ—
        longitude_col, latitude_col = _find_coordinate_columns(df)
        
        if longitude_col is None or latitude_col is None:
            available_columns = list(df.columns)
            return json.dumps({
                "status": "error", 
                "message": f"æ— æ³•è¯†åˆ«ç»çº¬åº¦åˆ—ã€‚å¯ç”¨åˆ—å: {available_columns}ã€‚æ”¯æŒçš„ç»åº¦åˆ—å: longitude, lon, lng, long, xã€‚æ”¯æŒçš„çº¬åº¦åˆ—å: latitude, lat, yã€‚"
            })
        
        print(f"   è¯†åˆ«åˆ°ç»åº¦åˆ—: '{longitude_col}', çº¬åº¦åˆ—: '{latitude_col}'")
        
        gdf = gpd.GeoDataFrame(
            df, geometry=gpd.points_from_xy(df[longitude_col], df[latitude_col])
        ).set_crs(epsg=4326)

        # ç¡®ä¿åæ ‡ç³»ä¸ºWeb Mercator (EPSG:3857) ä»¥ä¾¿åŒ¹é…åº•å›¾
        gdf = gdf.to_crs(epsg=3857)

        # åˆ›å»ºå›¾è¡¨
        # è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“ä¸ºé»‘ä½“
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

        fig, ax = plt.subplots(1, 1, figsize=(12, 12))

        # ä½¿ç”¨seabornçš„kdeplotåˆ›å»ºæ ¸å¯†åº¦ä¼°è®¡å›¾
        sns.kdeplot(
            x=gdf.geometry.x,
            y=gdf.geometry.y,
            fill=True,
            cmap="Reds",
            alpha=0.5,
            ax=ax
        )
        
        # æ·»åŠ åº•å›¾
        # å®šä¹‰å¹¶ä½¿ç”¨é«˜å¾·åœ°å›¾ä½œä¸ºåº•å›¾
        gaode_map_provider = {
            "url": "https://webrd01.is.autonavi.com/appmaptile?lang=zh_cn&size=1&scale=1&style=8&x={x}&y={y}&z={z}",
            "attribution": "Â© é«˜å¾·åœ°å›¾",
        }
        ctx.add_basemap(ax, source=gaode_map_provider['url'], crs=gdf.crs.to_string())

        # --- æ·»åŠ åœ°å›¾å…ƒç´  ---
        # 1. æ·»åŠ æ¯”ä¾‹å°º
        from matplotlib_scalebar.scalebar import ScaleBar
        ax.add_artist(ScaleBar(1, location='lower right'))

        # 2. æ·»åŠ æŒ‡åŒ—é’ˆ
        x, y, arrow_len = 0.95, 0.95, 0.07
        ax.annotate('N', xy=(x, y), xytext=(x, y - arrow_len),
                    arrowprops=dict(facecolor='black', width=4, headwidth=10),
                    ha='center', va='center', fontsize=20,
                    xycoords=ax.transAxes)

        # è®¾ç½®æ ‡é¢˜å’Œæ ·å¼
        ax.set_title(map_title, fontsize=16)
        ax.set_axis_off() # å…³é—­åæ ‡è½´

        # ä¿å­˜å›¾è¡¨
        plt.savefig(output_image_path, dpi=300, bbox_inches='tight')
        plt.close(fig) # å…³é—­å›¾è¡¨ä»¥é‡Šæ”¾å†…å­˜

        result = {
            "status": "success",
            "output_image_path": os.path.basename(output_image_path),  # åªè¿”å›žæ–‡ä»¶å
        }

    except ImportError as e:
        result = {"status": "error", "message": f"Missing required library: {e}. Please install geopandas, matplotlib, contextily, and seaborn."}
    except Exception as e:
        result = {"status": "error", "message": str(e)}

    return json.dumps(result)
def create_gif_from_images(image_files: list, output_gif_path: str = "animated_result.gif", fps: int = 2):
    """
    å°†ä¸€ç³»åˆ—è¾“å…¥çš„å›¾ç‰‡æ–‡ä»¶åˆæˆä¸ºä¸€ä¸ªGIFåŠ¨å›¾ã€‚

    :param image_files: ä¸€ä¸ªåŒ…å«è¾“å…¥å›¾ç‰‡æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ã€‚
    :param output_gif_path: è¾“å‡ºçš„GIFæ–‡ä»¶è·¯å¾„ã€‚
    :param fps: ç”Ÿæˆçš„GIFçš„å¸§çŽ‡ï¼ˆæ¯ç§’çš„å¸§æ•°ï¼‰ã€‚
    :return: åŒ…å«æ“ä½œçŠ¶æ€å’ŒGIFè·¯å¾„çš„JSONå­—ç¬¦ä¸²ã€‚
    """
    print(f"--- Pythonå‡½æ•° `create_gif_from_images` è¢«æ‰§è¡Œ ---")
    print(f"å‚æ•°: image_files={image_files}, output_gif_path='{output_gif_path}', fps={fps}")

    # ç¡®ä¿output_gif_pathä¿å­˜åˆ°outputsç›®å½•
    if not os.path.dirname(output_gif_path):
        output_gif_path = os.path.join(OUTPUT_DIR, output_gif_path)

    try:
        from PIL import Image

        if not image_files:
            return json.dumps({"status": "error", "message": "Image file list cannot be empty."})

        frames = [Image.open(f) for f in image_files]
        
        if not frames:
            return json.dumps({"status": "error", "message": "Could not open any images from the provided list."})

        frames[0].save(
            output_gif_path,
            save_all=True,
            append_images=frames[1:],
            duration=1000 / fps,
            loop=0
        )
        
        result = {"status": "success", "output_gif_path": os.path.basename(output_gif_path), "image_count": len(frames)}  # åªè¿”å›žæ–‡ä»¶å

    except ImportError:
        result = {"status": "error", "message": "Missing required library: Pillow (PIL). Please install it."}
    except FileNotFoundError as e:
        result = {"status": "error", "message": f"File not found: {e.filename}"}
    except Exception as e:
        result = {"status": "error", "message": str(e)}

    return json.dumps(result)
def visualize_clusters(input_shapefile: str, output_image_path: str = "cluster_visualization.png", map_title: str = "Cluster Analysis Visualization"):
    """
    æ ¹æ®è¾“å…¥çš„Shapefileç‚¹æ•°æ®ï¼ˆåŒ…å«'cluster'å­—æ®µï¼‰ï¼Œç”Ÿæˆä¸€å¼ å¸¦æœ‰åº•å›¾ã€æŒ‡åŒ—é’ˆå’Œæ¯”ä¾‹å°ºçš„å¯è§†åŒ–å›¾ã€‚

    :param input_shapefile: è¾“å…¥çš„Shapefileæ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»åŒ…å« 'cluster' å­—æ®µã€‚
    :param output_image_path: è¾“å‡ºçš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„ (PNGæ ¼å¼)ã€‚
    :param map_title: åœ°å›¾çš„æ ‡é¢˜ã€‚
    :return: åŒ…å«æ“ä½œçŠ¶æ€å’Œå›¾ç‰‡è·¯å¾„çš„JSONå­—ç¬¦ä¸²ã€‚
    """
    print(f"--- Pythonå‡½æ•° `visualize_clusters` è¢«æ‰§è¡Œ ---")
    print(f"å‚æ•°: input_shapefile='{input_shapefile}', output_image_path='{output_image_path}', map_title='{map_title}'")

    # ç¡®ä¿output_image_pathä¿å­˜åˆ°outputsç›®å½•
    if not os.path.dirname(output_image_path):
        output_image_path = os.path.join(OUTPUT_DIR, output_image_path)

    try:
        import geopandas as gpd
        import matplotlib.pyplot as plt
        import contextily as ctx
        from matplotlib_scalebar.scalebar import ScaleBar

        # è¯»å–Shapefile
        gdf = gpd.read_file(input_shapefile)
        if 'cluster' not in gdf.columns:
            return json.dumps({"status": "error", "message": "Input Shapefile must contain a 'cluster' column."})

        # ç¡®ä¿åæ ‡ç³»ä¸ºWeb Mercator (EPSG:3857)
        gdf = gdf.to_crs(epsg=3857)

        # --- ç»˜å›¾ ---
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        fig, ax = plt.subplots(1, 1, figsize=(12, 12))

        # æŒ‰ 'cluster' åˆ—å¯¹ç‚¹è¿›è¡Œåˆ†ç±»ç€è‰²
        gdf.plot(column='cluster', ax=ax, legend=True, markersize=10, cmap='tab20', categorical=True)
        
        # æ·»åŠ é«˜å¾·åº•å›¾
        gaode_map_provider = {
            "url": "https://webrd01.is.autonavi.com/appmaptile?lang=zh_cn&size=1&scale=1&style=8&x={x}&y={y}&z={z}",
            "attribution": "Â© é«˜å¾·åœ°å›¾",
        }
        ctx.add_basemap(ax, source=gaode_map_provider['url'], crs=gdf.crs.to_string())
        
        # --- æ·»åŠ åœ°å›¾å…ƒç´  ---
        # 1. æ·»åŠ æ¯”ä¾‹å°º
        ax.add_artist(ScaleBar(1, location='lower right'))

        # 2. æ·»åŠ æŒ‡åŒ—é’ˆ
        x, y, arrow_len = 0.95, 0.95, 0.07
        ax.annotate('N', xy=(x, y), xytext=(x, y - arrow_len),
                    arrowprops=dict(facecolor='black', width=4, headwidth=10),
                    ha='center', va='center', fontsize=20,
                    xycoords=ax.transAxes)

        # --- è®¾ç½®æ ‡é¢˜å’Œæ ·å¼ ---
        ax.set_title(map_title, fontsize=16)
        ax.set_axis_off()

        # ä¿å­˜å›¾è¡¨
        plt.savefig(output_image_path, dpi=300, bbox_inches='tight')
        plt.close(fig)

        result = {"status": "success", "output_image_path": os.path.basename(output_image_path)}  # åªè¿”å›žæ–‡ä»¶å

    except ImportError as e:
        result = {"status": "error", "message": f"Missing required library: {e}. Please install matplotlib-scalebar."}
    except Exception as e:
        result = {"status": "error", "message": str(e)}

    return json.dumps(result)
# ==============================================================================
# 2. ä¸ºLLMå®šä¹‰å·¥å…·çš„æè¿° (æ— éœ€ä¿®æ”¹)
# ==============================================================================
tools_description = [
    {
        "type": "function",
        "function": {
            "name": "preprocess_vehicle_data",
            "description": "æ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ç‚¹ç±»åž‹ï¼ˆèµ·ç‚¹æˆ–ç»ˆç‚¹ï¼‰ã€æ—¶é—´æˆ³èŒƒå›´æˆ–åœ°ç†ä½ç½®ï¼Œå¯¹è½¦è¾†è½¨è¿¹XLSXæ•°æ®è¿›è¡Œé¢„å¤„ç†å’Œç­›é€‰ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "filepath": { "type": "string", "description": "éœ€è¦å¤„ç†çš„æºæ•°æ®XLSXæ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ 'my_vehicle_data.xlsx'ã€‚"},
                    "point_type": { "type": "string", "description": "è¦ç­›é€‰çš„ç‚¹çš„ç±»åž‹ã€‚'start' ä»£è¡¨èµ·ç‚¹ (type=0)ï¼Œ'end' ä»£è¡¨ç»ˆç‚¹ (type=1)ã€‚", "enum": ["start", "end"]},
                    "start_time": { "type": "string", "description": "ç­›é€‰æ•°æ®çš„å¼€å§‹æ—¶é—´ã€‚å¯ä»¥æ˜¯ä»£è¡¨â€œå½“æ—¥ç§’æ•°â€çš„æ•´æ•°ï¼ˆå¦‚ '3600'ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯â€œHH:MM:SSâ€æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ '08:00:00'ï¼‰ã€‚"},
                    "end_time": { "type": "string", "description": "ç­›é€‰æ•°æ®çš„ç»“æŸæ—¶é—´ã€‚å¯ä»¥æ˜¯ä»£è¡¨â€œå½“æ—¥ç§’æ•°â€çš„æ•´æ•°ï¼ˆå¦‚ '7200'ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯â€œHH:MM:SSâ€æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆå¦‚ '09:30:00'ï¼‰ã€‚"},
                    "bbox": { "type": "array", "description": "åœ°ç†è¾¹ç•Œæ¡†ï¼Œä¸€ä¸ªåŒ…å«å››ä¸ªæ•°å­—çš„åˆ—è¡¨ï¼š[æœ€å°ç»åº¦, æœ€å°çº¬åº¦, æœ€å¤§ç»åº¦, æœ€å¤§çº¬åº¦]ã€‚", "items": {"type": "number"}}
                },
                "required": ["filepath"],
            },
        }
    },
    {
        "type": "function",
        "function": {
            "name": "kmeans_cluster",
            "description": "å¯¹åœ°ç†åæ ‡æ•°æ®è¿›è¡ŒK-Meansèšç±»åˆ†æžã€‚è‡ªåŠ¨è¯†åˆ«ç»çº¬åº¦åˆ—ï¼ˆæ”¯æŒlongitude/lon/lng/long/xå’Œlatitude/lat/yç­‰å¤šç§å‘½åæ–¹å¼ï¼‰ã€‚å¦‚æžœç”¨æˆ·æ²¡æœ‰æŒ‡å®šèšç±»æ•°é‡(n_clusters)ï¼Œä½ å¿…é¡»å‘ç”¨æˆ·æé—®ä»¥èŽ·å–æ­¤ä¿¡æ¯ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "input_filepath": { "type": "string", "description": "åŒ…å«ç»çº¬åº¦åˆ—çš„è¾“å…¥CSVæ–‡ä»¶çš„è·¯å¾„ã€‚å‡½æ•°ä¼šè‡ªåŠ¨è¯†åˆ«å¸¸è§çš„ç»çº¬åº¦åˆ—åï¼ˆå¦‚longitude/lon/lng/long/xå’Œlatitude/lat/yç­‰ï¼‰ã€‚é€šå¸¸æ˜¯æ•°æ®é¢„å¤„ç†æ­¥éª¤çš„è¾“å‡ºã€‚"},
                    "n_clusters": { "type": "integer", "description": "è¦å½¢æˆçš„èšç±»æ•°é‡ï¼ˆKå€¼ï¼‰ã€‚"},
                    "output_shapefile": { "type": "string", "description": "è¾“å‡ºçš„Shapefileæ–‡ä»¶çš„è·¯å¾„ï¼Œä¾‹å¦‚ 'clusters.shp'ã€‚"}
                },
                "required": ["input_filepath"],
            },
        }
    },
    {
        "type": "function",
        "function": {
            "name": "create_heatmap",
            "description": "åŸºäºŽè¾“å…¥çš„CSVç‚¹æ•°æ®ï¼Œç”Ÿæˆä¸€å¼ å¸¦æœ‰åœ¨çº¿åœ°å›¾èƒŒæ™¯çš„çƒ­åŠ›å›¾ï¼Œå¹¶ä¿å­˜ä¸ºPNGå›¾ç‰‡ã€‚è‡ªåŠ¨è¯†åˆ«ç»çº¬åº¦åˆ—ï¼ˆæ”¯æŒå¤šç§å‘½åæ–¹å¼ï¼‰ã€‚ç”¨äºŽåœ°ç†ç©ºé—´æ•°æ®çš„å¯è§†åŒ–åˆ†æžã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "input_filepath": { "type": "string", "description": "è¾“å…¥çš„ç‚¹æ•°æ®CSVæ–‡ä»¶è·¯å¾„ã€‚å‡½æ•°ä¼šè‡ªåŠ¨è¯†åˆ«å¸¸è§çš„ç»çº¬åº¦åˆ—åï¼ˆå¦‚longitude/lon/lng/long/xå’Œlatitude/lat/yç­‰ï¼‰ã€‚é€šå¸¸æ˜¯æ•°æ®é¢„å¤„ç†æ­¥éª¤çš„è¾“å‡ºã€‚"},
                    "output_image_path": { "type": "string", "description": "è¾“å‡ºçš„çƒ­åŠ›å›¾å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€‚ä¾‹å¦‚, 'heatmap.png'ã€‚"},
                    "map_title": { "type": "string", "description": "è¦æ˜¾ç¤ºåœ¨çƒ­åŠ›å›¾é¡¶éƒ¨çš„æ ‡é¢˜ã€‚"}
                },
                "required": ["input_filepath"],
            },
        }
    },
    {
        "type": "function",
        "function": {
            "name": "create_gif_from_images",
            "description": "å°†ä¸€ä¸ªå›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸­çš„æ‰€æœ‰å›¾ç‰‡åˆæˆä¸ºä¸€ä¸ªGIFåŠ¨å›¾ã€‚ç”¨äºŽåˆ›å»ºæ•°æ®éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€å¯è§†åŒ–ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "image_files": {
                        "type": "array",
                        "description": "ä¸€ä¸ªåŒ…å«æŒ‰é¡ºåºæŽ’åˆ—çš„ã€è¦è¢«åˆå¹¶æˆGIFçš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ã€‚",
                        "items": {"type": "string"}
                    },
                    "output_gif_path": { "type": "string", "description": "è¾“å‡ºçš„GIFæ–‡ä»¶çš„è·¯å¾„ã€‚ä¾‹å¦‚, 'animation.gif'ã€‚"},
                    "fps": { "type": "integer", "description": "GIFçš„å¸§çŽ‡ï¼ˆæ¯ç§’æ’­æ”¾çš„å›¾ç‰‡æ•°é‡ï¼‰ï¼Œå†³å®šäº†åŠ¨ç”»çš„é€Ÿåº¦ã€‚"}
                },
                "required": ["image_files", "output_gif_path"],
            },
        }
    },
    {
        "type": "function",
        "function": {
            "name": "visualize_clusters",
            "description": "ä¸ºK-Meansèšç±»çš„ç»“æžœï¼ˆä¸€ä¸ªShapefileï¼‰ç”Ÿæˆä¸€å¼ å¸¦æœ‰åº•å›¾ã€æŒ‡åŒ—é’ˆå’Œæ¯”ä¾‹å°ºçš„ã€æŒ‰ä¸åŒé¢œè‰²åŒºåˆ†ç°‡çš„å¯è§†åŒ–å›¾ç‰‡ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "input_shapefile": { "type": "string", "description": "è¾“å…¥çš„ç‚¹æ•°æ®Shapefileæ–‡ä»¶è·¯å¾„, å¿…é¡»åŒ…å«'cluster'åˆ—ã€‚é€šå¸¸æ˜¯kmeans_clusterå‡½æ•°çš„è¾“å‡ºã€‚"},
                    "output_image_path": { "type": "string", "description": "è¾“å‡ºçš„å¯è§†åŒ–å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€‚ä¾‹å¦‚, 'cluster_map.png'ã€‚"},
                    "map_title": { "type": "string", "description": "è¦æ˜¾ç¤ºåœ¨å›¾ç‰‡é¡¶éƒ¨çš„æ ‡é¢˜ã€‚"}
                },
                "required": ["input_shapefile"],
            },
        }
    }
]

# ==============================================================================
# 3. ä¸»æµç¨‹ï¼šä¸Ž DeepSeek V2 æ¨¡åž‹äº¤äº’
# ==============================================================================
def run_agent_conversation(user_prompt: str, messages: list = None):
    """
    è¿è¡Œä¸€ä¸ªå¯èƒ½åŒ…å«å¤šè½®å¯¹è¯çš„Agentæµç¨‹ã€‚

    :param user_prompt: ç”¨æˆ·çš„å½“å‰è¯·æ±‚å­—ç¬¦ä¸²ã€‚
    :param messages: ä¹‹å‰çš„å¯¹è¯åŽ†å²ã€‚å¦‚æžœä¸ºNoneï¼Œåˆ™å¼€å§‹ä¸€ä¸ªæ–°å¯¹è¯ã€‚
    :return: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¨¡åž‹çš„å›žç­”ã€æ˜¯å¦éœ€è¦ç»§ç»­å¯¹è¯ï¼Œä»¥åŠå½“å‰çš„å¯¹è¯åŽ†å²ã€‚
    """
    if messages is None:
        print("--- å¼€å¯æ–°å¯¹è¯ ---")
        # ä¸ºæ¨¡åž‹è®¾ç½®è§’è‰²å’Œè¡Œä¸ºå‡†åˆ™
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ã€å‹å¥½çš„åœ°ç†ç©ºé—´åˆ†æžAIåŠ©æ‰‹ã€‚"
            "ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·åˆ†æžåœ°ç†æ•°æ®ã€‚"
            "å½“ç”¨æˆ·çš„æŒ‡ä»¤ä¸æ˜Žç¡®æˆ–ç¼ºå°‘æ‰§è¡Œå·¥å…·æ‰€éœ€çš„å¿…è¦å‚æ•°æ—¶ï¼Œä½ å¿…é¡»å‘ç”¨æˆ·æé—®ä»¥æ¾„æ¸…é—®é¢˜ã€‚"
            "åœ¨è°ƒç”¨ä»»ä½•å·¥å…·ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å‚æ•°éƒ½å·²ä»Žç”¨æˆ·é‚£é‡ŒèŽ·å¾—ã€‚"
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    else:
        print("--- ç»§ç»­å¯¹è¯ ---")
        messages.append({"role": "user", "content": user_prompt})

    print(f"\nðŸ‘¤ ç”¨æˆ·: {user_prompt}\n")
    generated_files = []

    print("ðŸ¤– æ­£åœ¨å‘ DeepSeek V2 å‘é€è¯·æ±‚...")
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        tools=tools_description,
        tool_choice="auto",
    )
    response_message = response.choices[0].message
    tool_calls = response_message.tool_calls

    while True:
        if not tool_calls:
            final_answer = response_message.content
            # æ£€æŸ¥è¿™æ˜¯å¦æ˜¯ä¸€ä¸ªé—®é¢˜
            is_question = "?" in final_answer or "ï¼Ÿ" in final_answer
            if is_question:
                print(f"\nðŸ¤” æ¨¡åž‹æå‡ºé—®é¢˜: {final_answer}")
            else:
                print(f"\nâœ… DeepSeek V2 æœ€ç»ˆçš„å›žç­”:\n\n{final_answer}")
            
            messages.append({"role": "assistant", "content": final_answer})
            return {
                "answer": final_answer,
                "generated_files": list(set(generated_files)),
                "requires_follow_up": is_question,
                "messages": messages
            }

        print("âœ… DeepSeek V2 å†³å®šè°ƒç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå‡½æ•°ï¼")
        # å°†æ¨¡åž‹çš„å·¥å…·è°ƒç”¨å†³ç­–æ·»åŠ åˆ°åŽ†å²è®°å½•ä¸­
        messages.append(response_message.model_dump())
        
        # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            function_name = tool_call.function.name
            try:
                function_args = json.loads(tool_call.function.arguments)
                print(f"   - å‡½æ•°å: {function_name}")
                print(f"   - æ¨¡åž‹è§£æžå‡ºçš„å‚æ•°: {function_args}")

                available_functions = {
                    "preprocess_vehicle_data": preprocess_vehicle_data,
                    "kmeans_cluster": kmeans_cluster,
                    "create_heatmap": create_heatmap,
                    "create_gif_from_images": create_gif_from_images,
                    "visualize_clusters": visualize_clusters,
                }
                function_to_call = available_functions.get(function_name)
                
                if function_to_call:
                    function_response_str = function_to_call(**function_args)
                    response_data = json.loads(function_response_str)

                    # æ”¶é›†æ‰€æœ‰è¾“å‡ºçš„æ–‡ä»¶è·¯å¾„
                    for key, value in response_data.items():
                        if 'path' in key and isinstance(value, str):
                            generated_files.append(value)
                        elif key == 'generated_files' and isinstance(value, list):
                            generated_files.extend(value)
                    
                    if response_data.get("status") == "error":
                        print(f"âŒ å‡½æ•°æ‰§è¡Œå¤±è´¥: {response_data.get('message')}")
                    
                    # å°†æˆåŠŸçš„å·¥å…·æ‰§è¡Œç»“æžœæ·»åŠ åˆ°åŽ†å²è®°å½•
                    messages.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": function_name,
                        "content": function_response_str,
                    })
                else:
                    raise ValueError(f"æœªçŸ¥å‡½æ•°: {function_name}")

            except (json.JSONDecodeError, TypeError, ValueError, KeyError) as e:
                # å¦‚æžœå‚æ•°è§£æžå¤±è´¥æˆ–å‡½æ•°æ‰§è¡Œå‡ºé”™ï¼Œå‘æ¨¡åž‹æŠ¥å‘Šé”™è¯¯
                print(f"âŒ è°ƒç”¨å‡½æ•° '{function_name}' æ—¶å‡ºé”™: {e}")
                error_message = f"Error calling function {function_name}: {str(e)}. Please check your arguments."
                messages.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": function_name,
                    "content": json.dumps({"status": "error", "message": error_message}),
                })
                # è·³è¿‡æœ¬æ¬¡å¾ªçŽ¯ä¸­å‰©ä½™çš„å·¥å…·è°ƒç”¨ï¼Œè®©æ¨¡åž‹æ ¹æ®é”™è¯¯æŠ¥å‘Šå†³å®šä¸‹ä¸€æ­¥
                break
        
        print("\nðŸ”„ å·²æ‰§è¡Œæœ¬åœ°å‡½æ•°ï¼Œå°†ç»“æžœè¿”å›žç»™ DeepSeek V2 ä»¥å†³å®šä¸‹ä¸€æ­¥...")
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=messages,
            tools=tools_description,
            tool_choice="auto",
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

if __name__ == '__main__':
    # --- æ¨¡æ‹Ÿä¸€ä¸ªå¤šè½®å¯¹è¯åœºæ™¯ ---
    # 1. ç”¨æˆ·å‘èµ·ä¸€ä¸ªä¸å®Œæ•´çš„è¯·æ±‚
    initial_prompt = f"ä½ å¥½ï¼Œè¯·ä½¿ç”¨ 'SRTP/20200101_binjiang_point.xlsx' æ–‡ä»¶å¸®æˆ‘å¯¹èµ·ç‚¹æ•°æ®åšä¸ªèšç±»åˆ†æžã€‚"
    
    # 2. ç¬¬ä¸€æ¬¡è°ƒç”¨agent
    conversation_history = None
    result = run_agent_conversation(initial_prompt, conversation_history)
    conversation_history = result['messages']

    # 3. æ£€æŸ¥agentæ˜¯å¦éœ€è¦è¿½é—®
    if result['requires_follow_up']:
        print("\n--- éœ€è¦ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯ ---")
        # 4. æ¨¡æ‹Ÿç”¨æˆ·å›žç­”é—®é¢˜
        user_response = "å¥½çš„ï¼Œè¯·å¸®æˆ‘åˆ†æˆ5ç±»ã€‚"
        
        # 5. å¸¦ç€ç”¨æˆ·çš„å›žç­”å’Œå¯¹è¯åŽ†å²ï¼Œå†æ¬¡è°ƒç”¨agent
        result = run_agent_conversation(user_response, conversation_history)
        conversation_history = result['messages']

    # --- æœ€ç»ˆç»“æžœ ---
    print("\n\n===== å¯¹è¯ç»“æŸ =====")
    if not result['requires_follow_up']:
        print(f"æœ€ç»ˆå›žç­”: {result['answer']}")
        if result['generated_files']:
            print(f"ç”Ÿæˆçš„æ–‡ä»¶: {result['generated_files']}")
    else:
        print(f"å¯¹è¯æœªå®Œæˆï¼Œæ¨¡åž‹ä»åœ¨æé—®: {result['answer']}")

